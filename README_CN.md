# Awesome-Context-Compression-LLMs ğŸ—œï¸

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)

**[English](README.md)** | **[ä¸­æ–‡](README_CN.md)**

> ä¸€ä¸ªç²¾å¿ƒæ•´ç†çš„ç ”ç©¶è®ºæ–‡åˆé›†ï¼Œä¸“æ³¨äºé€šè¿‡ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•ˆç‡ã€‚è¿™äº›æ–¹æ³•æ—¨åœ¨**å‡å°‘Tokenä½¿ç”¨é‡**ã€**å‹ç¼©æ½œåœ¨çŠ¶æ€**ä»¥åŠ**ä¼˜åŒ–å†…å­˜å ç”¨ï¼ˆKVç¼“å­˜ï¼‰**ã€‚

## ğŸ“‘ ç›®å½•

- [ç®€ä»‹](#-ç®€ä»‹)
- [åˆ†ç±»ä½“ç³»](#-åˆ†ç±»ä½“ç³»)
- [æ˜¾å¼ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆæç¤º/è¾“å…¥å±‚é¢ï¼‰](#-æ˜¾å¼ä¸Šä¸‹æ–‡å‹ç¼©æç¤ºè¾“å…¥å±‚é¢)
- [éšå¼ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆæ½œåœ¨/æ¨ç†å±‚é¢ï¼‰](#-éšå¼ä¸Šä¸‹æ–‡å‹ç¼©æ½œåœ¨æ¨ç†å±‚é¢)
- [æ¨ç†æ—¶KVå‹ç¼©ï¼ˆå†…å­˜/ç¼“å­˜å±‚é¢ï¼‰](#-æ¨ç†æ—¶kvå‹ç¼©å†…å­˜ç¼“å­˜å±‚é¢)
- [ç»¼è¿°è®ºæ–‡](#-ç»¼è¿°è®ºæ–‡)
- [å¦‚ä½•è´¡çŒ®](#-å¦‚ä½•è´¡çŒ®)
- [Starå†å²](#-starå†å²)

## ğŸ¯ ç®€ä»‹

éšç€å¤§è¯­è¨€æ¨¡å‹å¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡å’Œæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œé«˜æ•ˆçš„ä¸Šä¸‹æ–‡ç®¡ç†å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬ä»“åº“æ ¹æ®å‹ç¼©å‘ç”Ÿçš„**ä½ç½®**å’Œ**æ–¹å¼**å°†è®ºæ–‡ç»„ç»‡ä¸ºä¸‰ä¸ªä¸åŒçš„ç±»åˆ«ï¼š

1. **æ˜¾å¼å‹ç¼©**ï¼šåœ¨ç¼–ç ä¹‹å‰/æœŸé—´å¯¹è¾“å…¥Tokenè¿›è¡Œæ“ä½œ
2. **éšå¼å‹ç¼©**ï¼šå‹ç¼©ä¸ºæ½œåœ¨è¡¨ç¤º
3. **KVå‹ç¼©**ï¼šåœ¨æ¨ç†æœŸé—´ä¼˜åŒ–é”®å€¼ç¼“å­˜

## ğŸ—‚ï¸ åˆ†ç±»ä½“ç³»

```
ä¸Šä¸‹æ–‡å‹ç¼©æ–¹æ³•
â”œâ”€â”€ æ˜¾å¼ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆè¾“å…¥å±‚é¢ï¼‰
â”‚   â”œâ”€â”€ Tokenå‰ªæï¼ˆLLMLingua, Selective-Contextï¼‰
â”‚   â”œâ”€â”€ åŸºäºæ‘˜è¦çš„æ–¹æ³•
â”‚   â””â”€â”€ ä¿¡æ¯è®ºé€‰æ‹©
â”‚
â”œâ”€â”€ éšå¼ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆæ½œåœ¨å±‚é¢ï¼‰
â”‚   â”œâ”€â”€ è½¯æç¤º / Gist Tokens
â”‚   â”œâ”€â”€ åŸºäºè‡ªç¼–ç å™¨ï¼ˆICAE, CoComï¼‰
â”‚   â””â”€â”€ æ½œåœ¨æ¨ç†ï¼ˆCoconutï¼‰
â”‚
â””â”€â”€ æ¨ç†æ—¶KVå‹ç¼©ï¼ˆç¼“å­˜å±‚é¢ï¼‰
    â”œâ”€â”€ é©±é€ç­–ç•¥ï¼ˆH2O, TOVA, SnapKVï¼‰
    â”œâ”€â”€ é‡åŒ–ï¼ˆKIVIï¼‰
    â””â”€â”€ ç¨€ç–æ³¨æ„åŠ›ï¼ˆStreamingLLMï¼‰
```

---

## ğŸ“ æ˜¾å¼ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆæç¤º/è¾“å…¥å±‚é¢ï¼‰

**å®šä¹‰**ï¼šä¸»è¦å¯¹è¾“å…¥æ–‡æœ¬æˆ–Tokenè¿›è¡Œæ“ä½œçš„æ–¹æ³•ã€‚å®ƒä»¬åœ¨**åˆå§‹ç¼–ç ä¹‹å‰æˆ–æœŸé—´**é€‰æ‹©ã€å‰ªææˆ–æ€»ç»“ä¸Šä¸‹æ–‡ï¼Œä»¥ç¼©çŸ­è¾“å…¥åºåˆ—é•¿åº¦ã€‚ç›®æ ‡é€šå¸¸æ˜¯åœ¨çª—å£å†…å®¹çº³æ›´å¤šä¸Šä¸‹æ–‡æˆ–é™ä½APIæˆæœ¬ã€‚

**å…³é”®è¯**ï¼š`æç¤ºå‹ç¼©`ã€`Tokenå‰ªæ`ã€`æ‘˜è¦`ã€`ä¿¡æ¯ç†µ`ã€`Tokené€‰æ‹©`ã€`ç²—ç²’åº¦å‰ªæ`

| è®ºæ–‡æ ‡é¢˜ | å‘è¡¨/æ—¥æœŸ | æ ‡ç­¾ | ä»£ç  | ç®€è¿° |
| :--- | :--- | :--- | :--- | :--- |
| [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression](https://arxiv.org/abs/2403.12968) | ACL 2024 | `å‰ªæ`, `è’¸é¦` | [GitHub](https://github.com/microsoft/LLMLingua) | ä»GPT-4æ ‡æ³¨ä¸­å­¦ä¹ å‹ç¼©ï¼Œæ¯”LLMLinguaå¿«3-6å€ |
| [LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression](https://arxiv.org/abs/2310.06839) | ACL 2024 | `å‰ªæ`, `RAG` | [GitHub](https://github.com/microsoft/LLMLingua) | é’ˆå¯¹RAGåœºæ™¯çš„é—®é¢˜æ„ŸçŸ¥å‹ç¼©ï¼ŒæŒ‰ç›¸å…³æ€§é‡æ’æ£€ç´¢æ–‡æ¡£ |
| [Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference](https://arxiv.org/abs/2403.09054) | MLSys 2024 | `å‰ªæ` | [GitHub](https://github.com/d-matrix-ai/keyformer) | åœ¨æ¯ä¸€å±‚è¯†åˆ«å…³é”®Tokenè¿›è¡Œé€‰æ‹©æ€§ä¿ç•™ |
| [In-context Autoencoder for Context Compression in a Large Language Model](https://arxiv.org/abs/2307.06945) | ICLR 2024 | `è‡ªç¼–ç å™¨` | [GitHub](https://github.com/getao/ICAE) | ä½¿ç”¨LoRAé€‚é…çš„LLaMAå°†é•¿ä¸Šä¸‹æ–‡å‹ç¼©åˆ°è®°å¿†æ§½ä¸­ |
| [RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation](https://arxiv.org/abs/2310.04408) | ICLR 2024 | `æ‘˜è¦`, `RAG` | [GitHub](https://github.com/carriex/recomp) | ä¸ºæ£€ç´¢æ–‡æ¡£è®­ç»ƒæŠ½å–å¼/ç”Ÿæˆå¼å‹ç¼©å™¨ |
| [Nugget: Neural Compression for Efficient Prompt Decoding](https://arxiv.org/abs/2310.04749) | ICLR 2024 | `å‰ªæ` | - | å­¦ä¹ è¯†åˆ«å’Œä¿ç•™"é‡‘å—"Tokenè¿›è¡Œå‹ç¼© |
| [Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading](https://arxiv.org/abs/2310.05029) | NAACL 2024 | `æ‘˜è¦` | - | MemWalkerï¼šè¿­ä»£æ€»ç»“å’Œå¯¼èˆªé•¿æ–‡æ¡£ |
| [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/abs/2310.05736) | EMNLP 2023 | `å‰ªæ` | [GitHub](https://github.com/microsoft/LLMLingua) | ä½¿ç”¨å°å‹LMè®¡ç®—å›°æƒ‘åº¦å¹¶å‰ªæä¿¡æ¯é‡å°‘çš„Tokenï¼Œå®ç°é«˜è¾¾20å€å‹ç¼© |
| [Selective-Context: Compressing Contexts for Efficient Inference](https://arxiv.org/abs/2310.06201) | EMNLP 2023 | `å‰ªæ`, `è‡ªä¿¡æ¯` | [GitHub](https://github.com/liyucheng09/Selective_Context) | ä½¿ç”¨å°å‹LMè¿‡æ»¤ä½è‡ªä¿¡æ¯å†…å®¹ |
| [Adapting Language Models to Compress Contexts](https://arxiv.org/abs/2305.14788) | EMNLP 2023 | `æ‘˜è¦` | [GitHub](https://github.com/princeton-nlp/AutoCompressors) | AutoCompressorï¼šé€’å½’å°†ç‰‡æ®µå‹ç¼©ä¸ºæ‘˜è¦å‘é‡ |
| [Learning to Compress Prompts with Gist Tokens](https://arxiv.org/abs/2304.08467) | NeurIPS 2023 | `è½¯æç¤º`, `Gist` | [GitHub](https://github.com/jayelm/gisting) | è®­ç»ƒ"gist tokens"å°†ä»»åŠ¡æŒ‡ä»¤å‹ç¼©ä¸º1-10ä¸ªè™šæ‹ŸToken |

---

## ğŸ§  éšå¼ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆæ½œåœ¨/æ¨ç†å±‚é¢ï¼‰

**å®šä¹‰**ï¼šå°†ä¸Šä¸‹æ–‡å‹ç¼©ä¸º**è½¯å‘é‡ã€åµŒå…¥æˆ–æ½œåœ¨çŠ¶æ€**çš„æ–¹æ³•ã€‚è¿™åŒ…æ‹¬å°†é•¿æ–‡æœ¬ç¼–ç ä¸ºç´§å‡‘çš„å‘é‡è¡¨ç¤ºï¼Œä»¥åŠ**æ½œåœ¨æ¨ç†**â€”â€”"æ€ç»´é“¾"æˆ–ä¸­é—´æ¨ç†æ­¥éª¤åœ¨æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œï¼ˆä¸è¾“å‡ºTokenï¼‰ä»¥å‡å°‘ç”Ÿæˆå¼€é”€ã€‚

**å…³é”®è¯**ï¼š`è½¯æç¤º`ã€`Gist Tokens`ã€`è‡ªç¼–ç å™¨`ã€`è®°å¿†å‘é‡`ã€`æ½œåœ¨ç©ºé—´æ¨ç†`ã€`è¿ç»­æ€ç»´é“¾`ã€`å†…éƒ¨çŠ¶æ€å‹ç¼©`

| è®ºæ–‡æ ‡é¢˜ | å‘è¡¨/æ—¥æœŸ | æ ‡ç­¾ | ä»£ç  | ç®€è¿° |
| :--- | :--- | :--- | :--- | :--- |
| [Coconut: Chain of Continuous Thought](https://arxiv.org/abs/2412.06769) | arXiv 2024.12 | `æ½œåœ¨æ¨ç†`, `CoT` | - | åœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†ï¼Œæ— éœ€è¾“å‡ºToken |
| [xRAG: Extreme Context Compression for Retrieval-Augmented Generation](https://arxiv.org/abs/2405.13792) | arXiv 2024.05 | `RAG`, `å‹ç¼©` | - | å°†æ£€ç´¢æ–‡æ¡£å‹ç¼©ä¸ºå¯†é›†è¡¨ç¤º |
| [PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents](https://arxiv.org/abs/2305.14564) | ACL 2024 | `è§„åˆ’`, `é•¿æ–‡æ¡£` | [GitHub](https://github.com/SimengSun/pearl) | å°†é•¿æ–‡æ¡£é—®ç­”åˆ†è§£ä¸ºè§„åˆ’å’Œæ‰§è¡Œ |
| [In-context Autoencoder for Context Compression in a Large Language Model](https://arxiv.org/abs/2307.06945) | ICLR 2024 | `è‡ªç¼–ç å™¨`, `è®°å¿†æ§½` | [GitHub](https://github.com/getao/ICAE) | ICAEå°†512ä¸ªTokenå‹ç¼©ä¸º128ä¸ªè®°å¿†æ§½ï¼Œå®ç°4å€å‹ç¼© |
| [Scaling Latent Reasoning via Thinking Tokens](https://arxiv.org/abs/2311.04254) | arXiv 2023.11 | `æ½œåœ¨æ¨ç†`, `æš‚åœToken` | - | ä½¿ç”¨"æ€è€ƒToken"è¿›è¡Œéšå¼æ¨ç†æ­¥éª¤ |
| [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/abs/2307.03170) | NeurIPS 2023 | `å¯¹æ¯”å­¦ä¹ `, `é•¿ä¸Šä¸‹æ–‡` | [GitHub](https://github.com/CStanKonrad/long_llama) | LongLLaMAï¼šä½¿ç”¨å¯¹æ¯”å­¦ä¹ èšç„¦ç›¸å…³ä¸Šä¸‹æ–‡ |
| [Learning to Compress Prompts with Gist Tokens](https://arxiv.org/abs/2304.08467) | NeurIPS 2023 | `Gist`, `æŒ‡ä»¤` | [GitHub](https://github.com/jayelm/gisting) | å°†æŒ‡ä»¤å‹ç¼©ä¸ºå¯å­¦ä¹ çš„gist tokens |
| [Adapting Language Models to Compress Contexts](https://arxiv.org/abs/2305.14788) | EMNLP 2023 | `é€’å½’`, `è½¯æç¤º` | [GitHub](https://github.com/princeton-nlp/AutoCompressors) | AutoCompressorï¼šé€’å½’å°†é•¿æ–‡æ¡£å‹ç¼©ä¸ºæ‘˜è¦å‘é‡ |
| [Parallel Context Windows for Large Language Models](https://arxiv.org/abs/2212.10947) | ACL 2023 | `å¹¶è¡Œ`, `è®°å¿†` | - | PCWï¼šåœ¨å¹¶è¡Œçª—å£ä¸­å¤„ç†ä¸Šä¸‹æ–‡å¹¶èšåˆ |
| [Training Language Models with Memory Augmentation](https://arxiv.org/abs/2205.12674) | EMNLP 2022 | `è®°å¿†`, `TRIME` | [GitHub](https://github.com/princeton-nlp/TRIME) | TRIMEï¼šåœ¨è®­ç»ƒæœŸé—´æ£€ç´¢å’Œæ•´åˆè®°å¿†Token |
| [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507) | ICLR 2020 | `å‹ç¼©`, `è®°å¿†` | - | ä½¿ç”¨å‹ç¼©è®°å¿†æ‰©å±•ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ |

---

## âš¡ æ¨ç†æ—¶KVå‹ç¼©ï¼ˆå†…å­˜/ç¼“å­˜å±‚é¢ï¼‰

**å®šä¹‰**ï¼šä¸“é—¨é’ˆå¯¹ç”Ÿæˆé˜¶æ®µ**é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜**çš„æ–¹æ³•ã€‚å®ƒä»¬æ—¨åœ¨é€šè¿‡é©±é€"ä¸é‡è¦"çš„KVå¯¹ã€é‡åŒ–ç¼“å­˜æˆ–ä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼æ¥å‡å°‘GPUå†…å­˜ä½¿ç”¨å’Œå»¶è¿Ÿã€‚è¿™äº›æ–¹æ³•é€šå¸¸åœ¨**æ¨ç†æœŸé—´å®æ—¶**å‘ç”Ÿã€‚

**å…³é”®è¯**ï¼š`KVç¼“å­˜é©±é€`ã€`Heavy Hitters`ã€`ç¨€ç–æ³¨æ„åŠ›`ã€`ç¼“å­˜é‡åŒ–`ã€`é¢„ç®—çº¦æŸç”Ÿæˆ`ã€`æ— é™ä¸Šä¸‹æ–‡`ã€`æµå¼æ¨ç†`

| è®ºæ–‡æ ‡é¢˜ | å‘è¡¨/æ—¥æœŸ | æ ‡ç­¾ | ä»£ç  | ç®€è¿° |
| :--- | :--- | :--- | :--- | :--- |
| [Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2406.10774) | ICML 2024 | `ç¨€ç–`, `æŸ¥è¯¢æ„ŸçŸ¥` | [GitHub](https://github.com/mit-han-lab/Quest) | åŸºäºé¡µé¢çš„KVç®¡ç†ä¸æŸ¥è¯¢æ„ŸçŸ¥é€‰æ‹© |
| [PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling](https://arxiv.org/abs/2406.02069) | arXiv 2024.06 | `é©±é€`, `åˆ†å±‚` | [GitHub](https://github.com/Zefan-Cai/PyramidKV) | ä¸åŒå±‚ä¿ç•™ä¸åŒæ•°é‡çš„KVå¯¹ï¼ˆé‡‘å­—å¡”ç»“æ„ï¼‰ |
| [MiniCache: KV Cache Compression in Depth Dimension for Large Language Models](https://arxiv.org/abs/2405.14366) | arXiv 2024.05 | `é©±é€`, `å±‚åˆå¹¶` | - | è·¨ç›¸ä¼¼å±‚åˆå¹¶KVç¼“å­˜ä»¥å‡å°‘å†…å­˜ |
| [SnapKV: LLM Knows What You are Looking for Before Generation](https://arxiv.org/abs/2404.14469) | arXiv 2024.04 | `é©±é€`, `è§‚å¯Ÿçª—å£` | [GitHub](https://github.com/FasterDecoding/SnapKV) | ä½¿ç”¨æç¤ºæœ«å°¾çš„è§‚å¯Ÿçª—å£è¯†åˆ«é‡è¦çš„KVå¯¹ |
| [CaM: Cache Merging for Memory-efficient LLMs Inference](https://arxiv.org/abs/2403.17696) | ICLR 2024 | `åˆå¹¶` | - | åˆå¹¶ç›¸ä¼¼çš„KVå¯¹è€Œéç¡¬æ€§é©±é€ |
| [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://arxiv.org/abs/2403.09636) | ICML 2024 | `å‹ç¼©`, `å­¦ä¹ ` | - | DMCï¼šå­¦ä¹ åŠ¨æ€å†³å®šä¿ç•™/ä¸¢å¼ƒä»€ä¹ˆ |
| [Gear: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference](https://arxiv.org/abs/2403.05527) | arXiv 2024.03 | `é‡åŒ–`, `æ®‹å·®` | [GitHub](https://github.com/HaoKang-Timmy/Gear) | é‡åŒ–å¤§éƒ¨åˆ† + ä½ç§©å¤„ç†å¼‚å¸¸å€¼ + ç¨€ç–æ®‹å·® |
| [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache](https://arxiv.org/abs/2402.02750) | ICML 2024 | `é‡åŒ–`, `2-bit` | [GitHub](https://github.com/jy-yuan/KIVI) | éå¯¹ç§°é‡åŒ–ï¼šé”®å’Œå€¼ä½¿ç”¨ä¸åŒçš„2-bitæ–¹æ¡ˆ |
| [Anchor-based Large Language Models](https://arxiv.org/abs/2402.07616) | ACL 2024 | `é”šç‚¹`, `å‹ç¼©` | [GitHub](https://github.com/lancopku/Anchor-LLM) | åˆ†ç»„å’Œé”šå®šTokenè¿›è¡Œå¹¶è¡Œå‹ç¼© |
| [InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences](https://arxiv.org/abs/2402.04617) | arXiv 2024.02 | `å—`, `è®°å¿†` | [GitHub](https://github.com/thunlp/InfLLM) | ä½¿ç”¨å—çº§è®°å¿†å•å…ƒå¤„ç†è¶…é•¿åºåˆ— |
| [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](https://arxiv.org/abs/2401.18079) | arXiv 2024.01 | `é‡åŒ–`, `é€šé“çº§` | [GitHub](https://github.com/SqueezeAILab/KVQuant) | é€šé“çº§é‡åŒ–ä¸å¼‚å¸¸å€¼å¤„ç†å®ç°æè‡´å‹ç¼© |
| [LoMA: Lossless Compressed Memory Attention](https://arxiv.org/abs/2401.09486) | arXiv 2024.01 | `å‹ç¼©`, `æ— æŸ` | - | é€šè¿‡é«˜æ•ˆå†…å­˜ç®¡ç†å®ç°æ— æŸå‹ç¼© |
| [TOVA: Token-wise Attention for Optimal KV-Cache Reduction](https://arxiv.org/abs/2401.06104) | arXiv 2024.01 | `é©±é€`, `Tokençº§` | [GitHub](https://github.com/schwartz-lab-NLP/TOVA) | æ ¹æ®æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­æ”¶åˆ°çš„æ³¨æ„åŠ›é©±é€Token |
| [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453) | ICLR 2024 | `æµå¼`, `æ³¨æ„åŠ›æ±‡` | [GitHub](https://github.com/mit-han-lab/streaming-llm) | StreamingLLMï¼šä¿ç•™åˆå§‹"æ±‡"Token + æœ€è¿‘çª—å£å®ç°æ— é™æµå¼æ¨ç† |
| [H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/abs/2306.14048) | NeurIPS 2023 | `é©±é€`, `Heavy Hitter` | [GitHub](https://github.com/FMInference/H2O) | ä»…ä¿ç•™"heavy-hitter" Tokenï¼ˆé«˜ç´¯ç§¯æ³¨æ„åŠ›ï¼‰åŠ æœ€è¿‘Token |
| [Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression](https://arxiv.org/abs/2305.17118) | NeurIPS 2023 | `é©±é€`, `æŒä¹…æ€§` | - | é‡è¦Tokenä¿æŒé‡è¦ï¼›åŸºäºå†å²é‡è¦æ€§å‰ªæ |
| [FastGen: Adaptive KV Cache Compression for Efficient LLM Inference](https://arxiv.org/abs/2310.01801) | arXiv 2023.10 | `é©±é€`, `è‡ªé€‚åº”` | - | åŸºäºæ³¨æ„åŠ›æ¨¡å¼çš„è‡ªé€‚åº”å‹ç¼©ç­–ç•¥ |

---

## ğŸ“š ç»¼è¿°è®ºæ–‡

| è®ºæ–‡æ ‡é¢˜ | å‘è¡¨/æ—¥æœŸ | å…³æ³¨ç‚¹ |
| :--- | :--- | :--- |
| [Prompt Compression for Large Language Models: A Survey](https://arxiv.org/abs/2410.12388) | NAACL 2025 | æç¤ºå‹ç¼©ç»¼åˆç»¼è¿° |
| [A Survey on Efficient Inference for Large Language Models](https://arxiv.org/abs/2404.14294) | arXiv 2024 | æ¶µç›–KVç¼“å­˜å’Œå…¶ä»–æ¨ç†ä¼˜åŒ– |
| [A Survey on Model Compression for Large Language Models](https://arxiv.org/abs/2308.07633) | TACL 2024 | é€šç”¨LLMå‹ç¼©ï¼ˆé‡åŒ–ã€å‰ªæã€è’¸é¦ï¼‰ |
| [Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models](https://arxiv.org/abs/2402.02244) | arXiv 2024 | ä¸“æ³¨äºæ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦ |
| [Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding](https://arxiv.org/abs/2312.17044) | arXiv 2023 | é•¿ä¸Šä¸‹æ–‡ä½ç½®ç¼–ç ç»¼è¿° |

---

## ğŸ¤ å¦‚ä½•è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

1. Forkæœ¬ä»“åº“
2. æŒ‰ç…§è¡¨æ ¼æ ¼å¼æ·»åŠ è®ºæ–‡
3. ç¡®ä¿è®ºæ–‡æ­£ç¡®åˆ†ç±»
4. æäº¤Pull Request

### åˆ†ç±»æŒ‡å—

- **ç±»åˆ«1ï¼ˆæ˜¾å¼ï¼‰**ï¼šå¦‚æœè®ºæ–‡è®¨è®ºåœ¨å‘é€åˆ°æ¨¡å‹/APIä¹‹*å‰*å‹ç¼©æç¤º
- **ç±»åˆ«2ï¼ˆéšå¼ï¼‰**ï¼šå¦‚æœè®ºæ–‡å‹ç¼©ä¸ºæ½œåœ¨å‘é‡æˆ–æ‰§è¡Œæ½œåœ¨æ¨ç†
- **ç±»åˆ«3ï¼ˆKVç¼“å­˜ï¼‰**ï¼šå¦‚æœè®ºæ–‡åœ¨æ¨ç†*æœŸé—´*é€šè¿‡æ“ä½œKVå¯¹æ¥ç®¡ç†GPUå†…å­˜

---

## â­ Starå†å²

[![Star History Chart](https://api.star-history.com/svg?repos=yourusername/Awesome-Context-Compression-LLMs&type=Date)](https://star-history.com/#yourusername/Awesome-Context-Compression-LLMs&Date)

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§[LICENSE](LICENSE)æ–‡ä»¶ã€‚

---

## ğŸ™ è‡´è°¢

ç‰¹åˆ«æ„Ÿè°¢æœ¬ä»“åº“ä¸­æ”¶å½•çš„æ‰€æœ‰ç ”ç©¶è€…ã€‚æ‚¨ä»¬ä¸ºä½¿LLMæ›´é«˜æ•ˆæ‰€åšçš„è´¡çŒ®æƒ åŠæ•´ä¸ªç¤¾åŒºã€‚

---

<p align="center">
  <i>å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªä»“åº“æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ç»™å®ƒä¸€ä¸ª â­ï¼</i>
</p>

